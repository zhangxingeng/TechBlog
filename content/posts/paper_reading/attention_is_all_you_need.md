---
date: 2023-11-17
title: "Understanding Transformers and self attention"
---

## Input and output
- Just like other recursive networks input is a sequence and output is also a sequence.
- Unlike other RNN, input should be provided before start
- Input should be vectors and output same. Translate back to original dataform as needed (like using word2vec)

## positional encoding
- Liturally adding a variant into the word vector so that word vectors of the same position are close together. (Question: since word embedding size is predetermined, and positional encoding can also be determined, why not append the PE at the end of input embedding? why add them together and lose the control of attention on position?)

## Multi-Head Attention
-self attention: 
- Query Key and Value: 
  - they are generated by their own network!
  - network `q` will take word embedding `x` as input and spit Q as query
  - similar network`k` and `v` will take `x` as input and spit `K` and `V` out
  - Assume at a party and everyone need to know about everyone to see if they are a potential teammate.
  - For each word x in the sentence as Questioned:
      - for each word x in the sentence again as Questioner:
        - x_1 is asked by all x_i. same for all x_i
        - By ask I mean dot product of Q of x_1 with K of all x_i. 
- this value of QxK is called the "attention" or we could say the matching score (whether two words are fitted)

Dimensions of Q, K, and V
Q and K Matching for Dot Product:

In the self-attention mechanism, the attention score between two words is calculated by taking the dot product of the query vector (Q) of one word with the key vector (K) of another word.
For the dot product to be computationally valid, the dimension of Q and K vectors that are being dotted together must match. This is a requirement of dot product operations: the vectors must be of the same length.
If Q is of size (5, 8) and K is of size (5, 8), you don't actually perform a dot product of these matrices as a whole. Instead, you calculate the dot product of each query vector (a row from Q) with each key vector (a row from K). This results in a (5, 5) matrix of attention scores, where each element represents the attention score between two words in your sequence.
Dimensional Relationship Between K and V:

The relationship between K and V is different. For each word, you use the attention scores (derived from Q and K) to create a weighted sum of the value vectors (V).
There's no requirement for K and V to have the same dimensionality in terms of their length. However, the number of rows in K and V should be the same since they correspond to the same words in your sequence.
If K is of size (5, 8) and V is of size (5, 10), for example, this is perfectly fine. The attention mechanism will use the (5, 5) attention score matrix to weight and sum the rows of V, resulting in an output matrix that has the same row count as V but retains the attention-weighted information.


- then these data is scaled (why?) and softmaxed
- 